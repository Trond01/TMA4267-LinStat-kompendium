



$\Bh$, $\B$, $\S$, $\Sh$, $\E$, $\Eh$

$\bh$, $\b$, $\s$, $\sh$, $\e$, $\eh$


theorem - trace formula 
\begin{theorem} \term{(Trace formula)}
    $$
        \E(\Y^T\bs{C}\Y) = \mr{tr}(\bs{C\Sigma}) + \M^T\bs{C}\M
    $$    
\end{theorem}
\begin{proof}
    \TODO{}
\end{proof}

theorem - ...


%%%%%%%%%%%%%%% Lecture 8 %%%%%%%%%%%%%%%
\subsection*{Lecture 8}

\begin{theorem}
    $\bs{Z}\sim N(0, I)$ and $\bs{R}$ symmetric and idempotent of rank $r$. Then 
    $$
        \bs{Z}^T \bs{R} \bs{Z} \sim \chi^2_r.
    $$
\end{theorem}


%%%%%%%%%%%%%%% Lecture 9 %%%%%%%%%%%%%%%
\subsection*{Lecture 9}

Assumptions
\begin{enumerate}
    \item $\X$ is of cull column rank
    \item $E\E=\boldsymbol{0}$
    \item Homostochastic: $\var(\e_i) = 0 \quad\forall i$.
    \item If $\X$ is random, then 2 and 3 are conditioned on $\X$. 
    \item Normality of errors: $\E\sim N(0, \s^2I_n)$.
\end{enumerate}

... obtain least squares estimators $\Bh, \Sh^2$ of $\B, \S^2$

Residuals ...

\subsubsection*{Parameter estimation}

Two approaches: LSE and MLE ...

$$
    \Bh = \argmin_{\B\in\R^{k+1}} \sum_{i=1}^n (Y_i - \x_i^T\B)^2
$$

... deducing that LSE and MLE give the same result ...

...

Hat matrix

ghjfiodeifgoerjfkdworw9u0gryhj

Du fulgte ikke med nei

%%%%%%%%%%%%%%% Lecture 10 %%%%%%%%%%%%%%%
\subsection*{Lecture 10}



%%%%%%%%%%%%%%% Lecture 11 %%%%%%%%%%%%%%%
\subsection*{Lecture 11}





%%%%%%%%%%%%%%% Lecture 12 %%%%%%%%%%%%%%%
\subsection*{Lecture 12}

questions about independence. Detour into sigma algebras etc ...

\begin{theorem}
    Suppose $X, Y$ are independent random variables and that $f, g$ are two measurable functions. Then $f(X), g(Y)$ are also independent. 
\end{theorem}

ANOVA - Analysis of variance

\begin{theorem} \term{(ANOVA decomposition)}\label{thm:ANOVA}
    Assuming the necesarry assumptions, 
    $$
    \underset{\mr{SST}}{\underbrace{
        \sum_{i=1}^n (Y_i - \bar{Y})}} = 
    \underset{\mr{SSR}}{\underbrace{
        \sum_{i=1}^n (\hat{Y}_i - \bar{Y})}} + 
    \underset{\mr{SSE}}{\underbrace{
        \sum_{i=1}^n (Y_i - \hat{Y}_i)^2}}. 
    $$
\end{theorem}
\begin{proof}
    TODO: there aint space in the margin
\end{proof}


R2 score \dots


%%%%%%%%%%%%%%% Lecture 13 %%%%%%%%%%%%%%%
\subsection*{Lecture 13}


\subsubsection*{Fictional model}
"Fictional model" using $x_{ij}$ as response for some fixed feature $j$. 

\input{text/tikz_figs/eq.tex}


... \TODO

\subsubsection*{General F-test}

We set up a much more general problem. Let $A\in\R^{r\times p}$, $r<p$, $\mathrm{rank}(A)=r$, $\boldsymbol{d}\in\R^d$. We test the hypothesis:
$$
    \mr{H}_0 : A\B = \bs{d}, 
    \quad\quad\quad
    \mr{H}_1 : A\B \ne \bs{d}.
$$
Some special cases of this general setup are.
\begin{enumerate}
    \item $r=1, d=0, A=(0, \dots, 1, \dots, 0)$ with $1$ at index $i$, gives the test 
    $$
        \mr{H}_0 : \b_i = 0, 
        \quad\quad\quad
        \mr{H}_1 : \b_i \ne 0.
    $$
    \item $r=1, d=0, A=(0, \dots, 1, \dots, -1, \dots, 0)$ with $1$ at index $i$ and $-1$ at index $j$, gives the test 
    $$
        \mr{H}_0 : \b_i = \b_j,
        \quad\quad\quad
        \mr{H}_1 : \b_i \ne \b_j.
    $$
    \item $r=k, d=\bs{0}\in\R^k, A=(\bs{0}, \mr{diag}(1))\in\R^{k\times p}$, gives the test 
    $$
        \mr{H}_0 : \b_i = 0 \quad\forall i\in\{1,\dots,k\},
        \quad\quad\quad
        \mr{H}_1 : \b_i \ne 0 \textrm{ for some } i\in\{1,\dots,k\}.
    $$
\end{enumerate}

%%%%%%%%%%%%%%% Lecture 14 %%%%%%%%%%%%%%%
\subsection*{Lecture 14}

Let $\mathcal{B}$ be the space of $\B$ satisfying $\mr{H}_0$. The restricted problem is:
$$
    \Bh^R = \argmin_{\B\in\mathcal{B}}(\Y-\X\B)^T(\Y-\X\B).
$$
Using lagrange multipliers and a bag of tricks, we obtain:
$$
    \Bh^R = \Bh - (\X^T\X)^{-1}\bs{A}^T(\bs{A}(\X^T\X)^{-1}A^T)^{-1}(\bs{A}\Bh-\bs{d}).
$$
Denoting $\Delta = \Bh-\Bh^R$, we find:
$$
    \mr{SSE}^R = \mr{SSE} + \Delta^T\X^T\X\Delta
$$

... IMPORTANT: the concrete expressions for the F statistic...

We claim that the under $\mr{H}_0$, we have

$$
    F = \frac{\mr{SSE}^R-\mr{SSE} / r}{\mr{SSE}/(n-p)} \sim F_{r, n-p}.
$$

\begin{proof}
    
    what the

\end{proof}




%%%%%%%%%%%%%%% Lecture 15 %%%%%%%%%%%%%%%
\subsection*{Lecture 15}


... example ...


\subsubsection*{Transformations of data}
Motivation:
...


box cox transformation
 

variance stabilising transformation

Suppose $\mu=\E(Y_i)$ and that $\var(Y_i)$ depends on $\mu$. ...

 

%%%%%%%%%%%%%%% Lecture 16 %%%%%%%%%%%%%%%
\subsection*{Lecture 16}

...






%%%%%%%%%%%%%%% Lecture 17 %%%%%%%%%%%%%%%
\subsection*{Lecture 17}
Suppose $k$ covariates. Then $2^k$ possible models from maximal:
$$
    Y_i = \b_0+\b_1x_{i1} + \dots + \b_kx_{ik}.
$$
to minimal:
$$
    Y_i = \b_0.
$$
We want to arrive at a compromise between simplisity and goodness of fit. 

\begin{enumerate}
    \item Adjusted coefficient of determination:
    $$
        R^2_{\mr{adj}} = 1 - \frac{\mr{SSE} / (n-k-1)}{\mr{SST} / (n-1)}
    $$
    \item 
    \item 
    \item 
\end{enumerate}
  

example...


\subsubsection*{Multiple hypothesis testing}

motivation ...

%%%%%%%%%%%%%%% Lecture 18 %%%%%%%%%%%%%%%
\subsection*{Lecture 18}

