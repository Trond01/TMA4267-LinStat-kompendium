\section{The Multivariate Normal Distribution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{From univariate to multivariate}
\subsubsection{Univariate case}

We begin by recalling important properties and results of the univariate normal distribution. Let $-\infty<\mu<\infty, \s\geq 0$. We say $X\sim N(\mu,\s^2)$ if it has density:
$$
    f_X(x) = \frac{1}{\sqrt{2\pi \s^2}} e^{-\frac{1}{2}\brac{\frac{x-\mu}{\sigma}}^2}.
$$
The case $\s=0$ is considered normal degenerate and we have $X\overset{\mathrm{a.s.}}{=}\mu$. One can show $\ev{X}=\mu, \var{X}=\s^2$ and that the moment generating and characteristic functions are:
\begin{align*}
    M_X(t) &= e^{\mu t + \frac{\s^2 t^2}{2}}, \\
    \phi_X(t) &= e^{i\mu t - \frac{\s^2 t^2}{2}}.
\end{align*}
We say that $Z\sim N(0, 1)$ is standard normal. We can transform as follows:
\begin{align*}
    X \sim N(\mu, \s^2) &\Rightarrow \frac{X-\mu}{\s} \sim N(0,1), \\
    Z \sim N(0,1) &\Rightarrow \s Z + \mu \sim N(\mu, \s^2).
\end{align*}
A final fundamental property is that linear combinations of independent normal random variables are also normal. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Multivariate case}

We choose to present one of several equivalent definitions. Here we first define a special case and then generalise it. 
\begin{definition}
    A p-variate random vector $\Z=(Z_1, \dots, Z_p)^\top$ is a \term{standard normal vector} if its components are independent and each $Z_i\sim N(0,1)$. We imediatelly obtain $f_{\Z}(\z) = \prod f_{Z_i} (z_i)$. We use the notation $Z\sim N(0, \mI)$.
\end{definition}
\begin{definition}
    We say that $\X = (X_1,\dots, X_p)^\top$ is \term{multivariate normal} if there exists a ($p\times q$) matrix $\mA$ and $\M\in\R^p$ such that 
    $$
        \X = \mA \Z + \M, \quad \Z \sim (0, \mI_q)
    $$
\end{definition}
\begin{theorem}
    For any vector $\mb\in\R^p$ we have that $\mb^\top \X$ is a univariate normal random variable.
\end{theorem}
\begin{proof}
    It is a linear combination of independent normal random variables:
    $$
        \mb^\top \X = \mb^\top(\mA\Z + \M) = \mb^\top \mA \Z + \mb^\top\M \sim N. 
    $$
\end{proof}
From the definition we find $\ev{X} = \M, \var{X} = \mA\mA^\top =: \SS$. We write $\X \sim N(\M, \SS)$. It turns out that the statement from the theorem would give an equivalent definition!
%\subsection*{Lecture 5}
\begin{theorem}
    Let $\XÂ \sim N(\M, \SS)$. If $\SS > 0$, then the probability density function (PDF) of $\X$ is:
    \begin{equation}
        \boxed{
            f_{\X} (\x) = \frac{1}{(2\pi)^{k/2}\det{\SS}^{1/2}} e^{-\frac{1}{2}(\x-\M)^\top\SS^{-1}(\x-\M)}
        }        
    \end{equation}
\end{theorem}
\begin{proof}
    Recall that $\X = \SS^{1/2}\Z + \M$. Using this, the result follows from the transformation formula and using the pdf of the standard normal $\Z$.  
\end{proof}
 
\begin{theorem}
    \label{thm:lin_comb}
    \begin{equation}
        \boxed{
            \X 
            \sim N(\M, \SS) \Rightarrow 
            \Y = 
            \mA\X+\boldsymbol{b} 
            \sim N(\mA\M+ \boldsymbol{b}, \mA \SS \mA^\top)
        }        
    \end{equation}
\end{theorem}
\begin{proof}
    Compute expected value and covariance. Normality follows from definition as we may write $\Y = \mA(\mB \Z + \boldsymbol{c}) + \boldsymbol{b} = \tilde{\mB} \Z + \tilde{\boldsymbol{b}}$. 
\end{proof}
The following corollary tells us that we may \term{standardize} in the multivariate case as well:
\begin{corollary}
    \begin{equation}
        \boxed{
            \X 
            \sim N(\M, \SS) \Rightarrow 
            \Z = 
            \SS^{-1/2}(\X-\M) 
            \sim N(0, \mI)
        }        
    \end{equation}
\end{corollary}
\begin{corollary}
    Any subvector $\X^*$ of $\X\sim N(\M, \SS)$ is also normal.
\end{corollary}
\begin{proof}
    Take $\mA$ to be a \hermetegn{component removing} matrix and use \Cref{thm:lin_comb}. 
\end{proof}

\begin{theorem}
    The characteristic function of $\X\sim N(\M, \SS)$ is:
    \begin{equation}
        \boxed{\phi_{\X}(\mt) = e^{ i \mt^\top\M - \frac{1}{2}\mt^\top\SS\mt}}
    \end{equation}
\end{theorem}
\begin{proof}
    For $Z\sim N(\zero, \mI_p)$, we may use independence to obtain:
    $$
        \phi_{\Z}(\mt) = \prod_{i=1}^p \phi_{Z_i}(t_i) = e^{-\frac{1}{2} \mt^\top\mt}.
    $$
    Then write $\X=\SS^{1/2} \Z + \M$ and rewrite:
    \begin{align*}
        \phi_{\X}(\mt) 
        &= \ev{e^{i\mt^\top\X}}
        = e^{i\mt^\top}\ev{e^{i\mt^\top\SS^{1/2}\Z}}
        = e^{i\mt^\top}\phi_{\Z}(\SS^{1/2}\mt)
        = e^{i\mt^\top}e^{-\frac{1}{2} \mt^\top\SS\mt}.
    \end{align*}
    Where we have used symmetry of $\SS^{1/2}$ a few times.
\end{proof}
The following is among the most important results in the course:
\begin{theorem}
    Suppose $\X\sim N(\M, \SS)$. Then 
    \begin{equation}
        \boxed{\mA \X, \mB \X \textrm{ are independent } \Leftrightarrow \cov{\mA \X, \mB \X} = \mA\SS\mB^\top = \zero}
    \end{equation} 
\end{theorem}
\begin{proof}
    The rightward implication is true for any random vectors. To prove the leftward for normal $\X$ we show that we may factor the characteristic function 
    $$
    \phi_{\begin{pmatrix}
        \mA \X \\ \mB \X
    \end{pmatrix}} (\mt) 
    = \phi_{\mA\X} (\mt_1) \phi_{\mB\X} (\mt_2).
    $$
    %\TODO{details if time, lecture 6}
\end{proof}
\begin{corollary}
    Let $X = (X_1,\dots,X_p)^\top\sim N{\M, \SS}$. Then $X_i, X_j$ independent iff $\cov{X_i,X_j}=0$.
\end{corollary}
The next theorem tells us that we may \hermetegn{remove the dependent part} of a component from another:
\begin{theorem}
    Let $\X\sim N(\M, \SS)$ be $p$-variate and $\X=(\X_1,\X_2)^\top$ with $\X_i\sim N(\M_i, \SS_i)$. Define $\X_2' = \X_2 - \SS_{21}\SS_{11}^{-1} \X_1$ Then:
    \begin{enumerate}
        \item $X_2'\sim N(\M_2 - \SS_{21}\SS_{11}^{-1}\M_1, \SS_{22} - \SS_{21}\SS_{11}^{-1}\SS_{12})$.
        \item $\X_1, \X_2'$ are independent.
    \end{enumerate}
\end{theorem}

\begin{theorem}
    With notation as in the previous theorem, the conditional distribution of $\X_2$ given $\X_1=\x_1$ is:
    $$
        \X_2 |_{\X_1 = \x_1} \sim N(\M_2 + \SS_{21}\SS_{11}^{-1}(\x_1-\M_1), \SS_{22}-\SS_{21}\SS_{11}^{-1}\SS_{12}).
    $$
\end{theorem}

\begin{theorem}
    If $\X\sim N(\M, \SS)$ and $\SS$ is non-singular. Then 
    $$
        \boxed{U = (\X-\M)\SS^{-1}(\X-\M) \sim \chi^2_p}
    $$
\end{theorem}
\begin{proof}
    By definition of the $\chi^2$-distribution we have $U=\Z^\top\Z=Z_1^2+\dots+Z_p^2 \sim \chi_p^2$ for $\Z\sim N(0, \mI_p)$. By the \term{Mahalanobis transformation} we have $\Z=\SS^{\frac{1}{2}}(\X-\M)$ from which the result follows. 
\end{proof}


%\subsection*{Lecture 7}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Estimation of the multivariate normal distribution}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Univariate case}
From the univatiate case we recall that if $X_1,\dots,X_n \sim N(\mu, {\s^2})$ are independent, then the MLE estimators are:
\begin{align*}
    \mh &= \bar{X} = \frac{1}{n} \sum_{i=1}^n X_i, \\
    \sh^2 &= \frac{1}{n} \sum_{i=1}^n (X_i-\bar{X})^2.
\end{align*}
But since we found that $\ev{\s^2}=\frac{n-1}{n} \s^2$ is biased, we use instead the estimator
$$
    S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i-\bar{X})^2.
$$
We further proved that
\begin{enumerate}
    \item $\bar{X} \sim N(\mu, \s^2/n)$,
    \item $\frac{(n-1)S^2}{\s^2} \sim \chi^2_{n-1}$,
    \item $\bar{X}, S^2$ are independent (for the normal distribution),
    \item $\sqrt{n} \frac{\bar{X}-\mu}{S} \sim t_{n-1}$ (student t distribution).
\end{enumerate}
Our next goal is to obtain the result for the multivariate case. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Multivatiate case}
In this case, we have $p$-variate independent randum vectors $\X_1,...,\X_n \sim N(\M, \SS)$ where we denote $\X_i=(X_{i1},\dots,X_{ip})^\top$. These make up the columns of the ($n\times p$) \term{data matrix} or \term{feature matrix} $\X$ given as:
$$
    \X = 
    \underset{\mr{features} \rightarrow}{
    \begin{pmatrix}
        X_{11} & \cdots & X_{1p} \\
        \vdots & \ddots & \vdots \\
        X_{n1} & \cdots & X_{np}
    \end{pmatrix}}
    \substack{\text{samples} \\ \downarrow}
    = \begin{pmatrix}
        \X_1^\top \\ \vdots \\ \X_n^\top
    \end{pmatrix}= (\X_1 \cdots \X_n)^\top.
$$
We want to estimate $\M, \SS$. Again we denote:
\begin{align*}
    \Mh &= \bar{\X} = \frac{1}{n} \sum_{i=1}^n \X_i = \frac{1}{n}\X^\top \one, \\
    \bs{S}^2 
    &= \frac{1}{n} \sum_{i=1}^n (\X_i-\bar{\X})^\top(\X_i-\bar{\X}) 
    = \frac{1}{n} \X^\top\brac{\mI-\frac{1}{n} \one\one^\top}\X.
\end{align*}
The matrix $\mC=\mI-\frac{1}{n} \one\one^\top$ is called the \term{centering matrix} because its action is to remove the mean of a vector:
$$
    \mC \y = \begin{pmatrix}
        1-\frac{1}{n} & \cdots & \frac{1}{n} \\
        \vdots & \ddots & \vdots \\
        \frac{1}{n} & \cdots & 1-\frac{1}{n}
    \end{pmatrix}
    \begin{pmatrix}
        y_1 \\ \vdots \\ y_n
    \end{pmatrix}
    = 
    \begin{pmatrix}
        y_1-\bar{y} \\ \vdots \\ y_n-\bar{y}
    \end{pmatrix}.
$$
We note that 
\begin{enumerate}
    \item $\mC$ is symmetric
    \item $\mC$ is idempotent
\end{enumerate}
For the estimators, we may prove:
\begin{proposition}
    $\bar{\X} \sim N(\M, \frac{1}{n}\SS)$
\end{proposition}
\begin{proof}
    The proof is by factoring the characteristic function. We can do this since the $\X_i$'s are independent.
    \begin{align*}
        \varphi_{\bar{\X}}(\mt) 
        &= \ev{e^{i \mt^\top \bar{\X}}}
        = \ev{ e^{i \left(\frac{\mt}{n}\right)^\top (\X_1, \ldots, \X_n)} }
        = \phi_{\X_1+\dots+\X_n} \left(\frac{\mt}{n}\right)
        = \varphi_{\X_1} \left( \frac{\mt}{n} \right) \cdots \varphi_{x_n} \left( \frac{\mt}{n} \right)
        \\&= \prod_{i=1}^{n} e^{ i \left(\frac{\mt}{n}\right)^\top\M - \frac{1}{2} \left(\frac{\mt}{n}\right)^\top \SS \left(\frac{\mt}{n}\right)^\top }
        = e^{\sum_{j=1}^{n} i \left(\frac{\mt}{n}\right)^\top\M - \frac{1}{2} \left(\frac{\mt}{n}\right)^\top \SS \left(\frac{\mt}{n}\right)^\top}
        = e^{ i \mt^\top \mu - \frac{1}{2} \mt^\top \frac{\SS}{n} \mt }.
    \end{align*}

\end{proof}
\begin{proposition}
    $\ev{\bs{S}} = \frac{n-1}{n} \SS.$
\end{proposition}
\begin{proof}
    This is a more straigh forward computation.
\end{proof}
Hence we obtain an unbiased estimator as
$$
    \bs{S} = \frac{1}{n-1} \sum_{i=1}^n (\X_i-\bar{\X})(\X_i-\bar{\X})^\top.
$$

%\subsection*{Lecture 8}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Quadratic forms}
Let $\X=(X_1,\dots,X_p)^\top$ be a ($p\times 1$) random vector and $\mA=(a_ij)\in\R^{p\times p}$ a matrix. This gives a \term{quadratic form}:
$$
    \X^\top\mA\X = \sum_{i,j} X_ia_{ij}X_j.
$$ 
\begin{theorem} Suppose $\X\sim (\M, \SS)$ then the \term{trace formula} holds:
    \begin{equation}
        \boxed{\ev{\X^\top\mA\X} = \mr{tr}(\bs{A\Sigma}) + \M^\top\mA\M
       } 
    \end{equation}
\end{theorem} 
\begin{proof}
    Using that $\cov{X_i,X_j} = \ev{X_iX_j}-\ev{X_i}\ev{X_j}$ we obtain the result:
    \begin{align*}
        \ev{\X^\top\mA\X}
        &= \sum_{i,j}a_{ij}\ev{X_iX_j}
        = \sum_{i,j}a_{ij}(\Sigma_{ij}-\mu_i\mu_j)
        \\&= \sum_{i,j}a_{ij}\Sigma_{ij} -  \sum_{ij}a_{ij}\mu_i\mu_j
        = \mr{tr}(\mA\SS) - \M^\top\mA\M.
    \end{align*}
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Idempotent matrices}
Recall that the (square) matrix $\mA$ is said to be idempotent of $\mA\mA=\mA$. We have the following:
\begin{proposition}
    Let $\mA$ be idempotent.
    \begin{enumerate}
        \item $\mI-\mA$ is also idempotent.
        \item $\mA(\mI-\mA)=(\mI-\mA)\mA=\zero$.
        \item The only non-singular idempotent matrix is $\mI$.
        \item All eigenvalues of idempotent matrices are $0$ or $1$.
    \end{enumerate}
\end{proposition}
\begin{proof}
    We have:
    \begin{enumerate}
        \item $(\mI-\mA)^2=\mI^2-\mA\mI-\mI\mA+\mA^2=0$.
        \item Obvious
        \item Suppose $\mA$ is non-singular. Then $\mI=\mA^{-1}\mA=\mA^{-1}\mA\mA=\mA$.
        \item $\lambda\x=\mA\x=\mA\mA\x=\lambda^2\x$.
    \end{enumerate}
\end{proof}
% Before our next important result, we recall from linear algebra that:
% \begin{enumerate}
%     \item Let $\mA\in\R^{p\times p}$ have eigenvalues $\lambda_1,\dots,\lambda_p$. Then:
%         \begin{align*}
%             \mr{tr}(\mA) = \sum \lambda_i, \quad
%             \mr{det}(\mA) = \prod \lambda_i.
%         \end{align*}
%     \item The matrix $\mA\in\R^{p\times p}$ of rank $r$ has \term{spectral decomposition}:
%         $$
%             \mA = \bs{P}\mL\bs{P}^\top
%         $$
%         where $P$ is the matrix with eigenvalues as columns and $\mL$ the diagonal matrix of eigenvalues. Note that $\bs{P}\in\R^{p\times r}, \mL\in\R^{r\times r}$.
%         \TODO{something about orthonormal}
% \end{enumerate}
Let $\mA=\boldsymbol{P}\boldsymbol{\Lambda}\boldsymbol{P}^\top\in\R^{p\times p}$ be symmetric and idempotent. Since only non-zero eigenvalues are $1$, we have $\mL=\mI_r$.
% in point 2 
Also $\rank{\mA} = \rank{\boldsymbol{\Lambda}} = r$.
% $\mr{rank}(\mA)=\mr{tr}(\mA)=\tr{\boldsymbol{P}\boldsymbol{\Lambda}\boldsymbol{P}^\top}=\tr{\boldsymbol{\Lambda}\boldsymbol{P}^\top\boldsymbol{P}}=\tr{\boldsymbol{\Lambda}}=r$.
% due to point 1. 

The following theorem will be used extensivelly in the sequel. 
\begin{theorem}
    $\bs{Z}\sim N(0, \mI_p)$ and let $\bs{R}, \bs{S}$ be symmetric and idempotent of rank $r, s$ respectivelly. Suppose also $\bs{RS}=0$. Then 
    \begin{enumerate}
        \item $\bs{Z}^\top \bs{R} \bs{Z} \sim \chi^2_r$
        \item $\bs{Z}^\top \bs{R} \bs{Z}$ and $\bs{Z}^\top \bs{S} \bs{Z}$ are independent
        \item $\frac{\bs{Z}^\top \bs{R} \bs{Z} / r}{\bs{Z}^\top \bs{S} \bs{Z}/s} \sim F_{r,s}$.
    \end{enumerate}
\end{theorem}
\begin{proof} We have:
    \begin{enumerate}
        \item Use spectral decomposition $\bs{R} = \bs{P}\mI_r \bs{P}^\top$ to find:
        $$
            \Z^\top\bs{R}\Z 
            = (\bs{P}^\top\Z)^\top \mI_r \bs{P}^\top\Z
            = \Y^\top \Y.
        $$
        Note that $\Y=\bs{P}^\top\Z$ is $r$-variate normal and compute its expectation and variance to be $0$ and $\mI_r$. Then finally conclude that $\Y^\top \Y^\top\Y=Y_1^2+\dots+Y_r^2\sim\chi^2_r$.
        \item Compute $\cov{\bs{R}\Z, \bs{S}\Z} = \bs{R}\bs{S} = \zero$. Hence $\bs{R}\Z, \bs{S}\Z$. Then the measurable function $f(\x)=\x^\top \x$ gives independence of $\bs{Z}^\top \bs{R} \bs{Z}$ and $\bs{Z}^\top \bs{S} \bs{Z}$.
        \item By definition of Fisher distribution.
    \end{enumerate}
\end{proof}


