


%%%%%%%%%%%%%%% Lecture 7 %%%%%%%%%%%%%%%
\subsection*{Lecture 7}

\subsection{Estimation of the multivariate normal distribution}
\subsubsection{Univariate case}
From the univatiate case we recall that if $X_1,\dots,X_n \sim N(\mu, {\s^2})$ are independent, then the MLE estimators are:
\begin{align*}
    \mh &= \bar{X} = \frac{1}{n} \sum_{i=1}^n X_i \\
    \s^2 &= \frac{1}{n} \sum_{i=1}^n (X_i-\bar{X})^2
\end{align*}
But since we found that $\ev{\s^2}=\frac{n-1}{n} \s^2$ is biased, we use instead the estimator
$$
    S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i-\bar{X})^2.
$$
We further proved that
\begin{enumerate}
    \item $\bar{X} \sim N(\mu, \s^2/n)$
    \item $\frac{(n-1)S^2}{\s^2} \sim \chi^2_{n-1}$
    \item $\bar{X}, S^2$ are independent (for the normal distribution)
    \item $\sqrt{n} \frac{\bar{X}-\mu}{S} \sim t_{n-1}$ (student t distribution)
\end{enumerate}
Our next goal is to obtain the result for the multivariate case. 
\subsubsection{Multivatiate case}
In this case, we have $p$-variate independent randum vectors $\X_1,...,\X_n \sim N(\M, \SS)$ where we denote $\X_i=(X_{i1},\dots,X_{ip})^T$. These make up the columns of the ($n\times p$) \term{data matrix} or \term{feature matrix} $\X$ given as:
$$
    \X = 
    \underset{\mr{samples} \rightarrow}{
    \begin{pmatrix}
        X_{11} & \cdots & X_{n1} \\
        \vdots & \ddots & \vdots \\
        X_{1p} & \cdots & X_{np}
    \end{pmatrix}}
    \substack{\text{features} \\ \downarrow}
    = (\X_1 \cdots \X_n).
$$
We want to estimate $\M, \SS$. Again we denote:
\begin{align*}
    \Mh &= \bar{\X} = \frac{1}{n} \sum_{i=1}^n \X_i = \frac{1}{n}\X^T \one \\
    \bs{S}^2 
    &= \frac{1}{n} \sum_{i=1}^n (\X_i-\bar{\X})^T(\X_i-\bar{\X}) 
    = \frac{1}{n} \X^T\brac{\mI-\frac{1}{n} \one\one^T}\X.
\end{align*}
The matrix $\mC=\mI-\frac{1}{n} \one\one^T$ is called the \term{centering matrix} because
$$
    \mC \y = \begin{pmatrix}
        1-\frac{1}{n} & \cdots & \frac{1}{n} \\
        \vdots & \ddots & \vdots \\
        \frac{1}{n} & \cdots 1-\frac{1}{n}
    \end{pmatrix}
    \begin{pmatrix}
        y_1 \\ \vdots \\ y_p
    \end{pmatrix}
$$








dette er en test

$\Bh$, $\B$, $\S$, $\Sh$, $\E$, $\Eh$

$\bh$, $\b$, $\s$, $\sh$, $\e$, $\eh$


theorem - trace formula 
\begin{theorem} \term{(Trace formula)}
    $$
        \E(\Y^T\bs{C}\Y) = \mr{tr}(\bs{C\Sigma}) + \M^T\bs{C}\M
    $$    
\end{theorem}
\begin{proof}
    \TODO{}
\end{proof}

theorem - ...


%%%%%%%%%%%%%%% Lecture 8 %%%%%%%%%%%%%%%
\subsection*{Lecture 8}

\begin{theorem}
    $\bs{Z}\sim N(0, I)$ and $\bs{R}$ symmetric and idempotent of rank $r$. Then 
    $$
        \bs{Z}^T \bs{R} \bs{Z} \sim \chi^2_r.
    $$
\end{theorem}