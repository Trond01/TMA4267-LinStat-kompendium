\section{Multivariate Distribution and its Generalisations}
%%%%%%%%%%%%%%% Lecture 1 %%%%%%%%%%%%%%%
\subsection*{Lecture 1}

\term{random vector} - vector with RV's as components

\term{random matrix}

\term{cumulative distribution function} (CDF)

$$
    F(x)=\p{\X\leq \x} = \p{X_1\leq x_1,\dots,X_p\leq x_p}
$$

\term{absolutelly continuous} if there exists density function $f$ such that:

$$
    F(\x)=\int_{-\infty}^{x_p}\dots\int_{-\infty}^{x_1} f(u_1,\dots,u_p) du_1\dots du_p
$$

Then 

$$
    \p{\X\in D} = \int_D f(\x) d\x \quad\forall D\subseteq \R^p
$$

$\X$ is said to be discrete if it is consentraded on a countable (finite or infinite) set of points. Then integral becomes a sum. In the absolutelly continuous case, we may write:

$$
    f(\x)=f\left(x_1, \ldots, x_p\right)=\frac{\partial^p F\left(x_1, \ldots, x_p\right)}{\partial x_1 \cdots \partial x_p}
$$

\textbf{\term{Marginal distribution}} 

Let $\X_A, \X_B$ be two random vectors st $\X=(\X_A,\X_B)^\top$ has cdf $F$. Then:

$$
    F_A(x_1,\dots,x_k) = F(x_1,\dots,x_k,\infty,\dots,\infty)
$$

In absolutelly continuous case we find

$$
    f_A(x_1,\dots,x_k) = \int_{-\infty}^{\infty}\dots\int_{-\infty}^{\infty} f(x_1,\dots,x_p) du_p\dots du_{k+1}
$$

\textbf{\term{Conditional distribution}} 

$$
    f_{\X_B|\X_A=\x_A} = \frac{f(x_1,\dots,x_p)}{f_A(x_1,\dots,x_k)}
$$

\textbf{\term{Independence}} 

Say $\X_A, \X_B$ are independent if 

$$
    F(x_1,\dots,x_p)=F_A(x_1,\dots,x_k)F_B(x_{k+1},\dots,x_p) \qall x_1,\dots x_p.
$$

In the continuous case we have independence iff $f=f_A\cdot f_B$. In this case $f(\x_B|\x_A) = f_B(\x_B)$.

Similar definition for independence when $\X$ has $N$ components and not just 2.

\textbf{\term{Multivariate expectations and moments}}

\term{expectation} defined as

$$
    \ev{\X} = (\ev{X_1},\dots,\ev{X_p})^\top
$$
%%%%%%%%%%%%%%% Lecture 2 %%%%%%%%%%%%%%%
\subsection*{Lecture 2}

We can show
$
    \ev{a \X + b \Y} = a\ev{\X}+ b \ev{\Y}
$

For (shape compatible) matrices $\mA, \mB$ we have
$
    \ev{\mA\X \mB} = \mA\ev{\X}\mB
$

Let $\X, \Y$ be random matrices whose product is defined. Then $\ev{\X\Y} = \ev{\X}\ev{\Y}$.

%%%%%%%%%%%%%%%
\textbf{\term{Covariance matrix}}

Let $\X=(X_1,\dots,X_p)^\top$ and $\ev{\X}=:\M=({\mu_1,\dots,\mu_p})$. We then define:
$$
    \var{\X} = \cov{\X} = \SS = \begin{pmatrix}
        \sigma_{X_1X_1} & \dots & \sigma_{X_1X_p} \\
        \vdots & \ddots & \vdots \\
        \sigma_{X_pX_1} & \dots & \sigma_{X_pX_p}
    \end{pmatrix}
    =\ev{(\X-\M)(\X-\M)^\top}
$$
This natrix is \term{symmetric}. We can also show:
$$
    \SS = \ev{\X\X^\top} - \M\M^\top.
$$
The correlation matrix (with ones on the diagonal) is given by
$$
    \boldsymbol{\rho}
    =
    \begin{pmatrix}
        \rho_{X_1X_1} & \dots & \rho_{X_1X_p} \\
        \vdots & \ddots & \vdots \\
        \rho_{X_pX_1} & \dots & \rho_{X_pX_p}
    \end{pmatrix}
    ,\quad
    \rho_{X_iX_j} = \frac{\sigma_{X_iX_j}}{\sqrt{\sigma_{X_i}}\sqrt{\sigma_{X_j}}}
$$
For two random vectors $\X,\Y$ we define their correlation matrix by
$$
    \SS_{\X\Y} 
    = \cov{\X,\Y} 
    = \ev{(\X-\M_{\X})(\Y-\M_{\Y})^\top}
    =(\cov{X_i,X_j})_{\substack{i=1,\dots,p \\ j= 1,\dots,q}}
$$

\begin{proposition}
    The covariance matrix $\SS$ is positive semi-definite.
\end{proposition}
\begin{proof}
    Using the formula for the variance of a linear combination we obtain:
    \begin{align*}
        \y^\top \SS \y 
        &= 
        \begin{pmatrix}
        y_1 & \cdots & y_n
        \end{pmatrix}
        \begin{pmatrix}
        \Sigma_{11} & \Sigma_{12} & \cdots & \Sigma_{1n} \\
        \Sigma_{21} & \Sigma_{22} & \cdots & \Sigma_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        \Sigma_{n1} & \Sigma_{n2} & \cdots & \Sigma_{nn}
        \end{pmatrix}
        \begin{pmatrix}
        y_1 \\
        y_2 \\
        \vdots \\
        y_n
        \end{pmatrix}
        = \sum_{i=1}^{n} \sum_{j=1}^{n} y_i \Sigma_{ij} y_j
        \\&= \sum_{i=1}^{n} y_i^2 \text{Var}(X_i) + 2 \sum_{i < j} y_i y_j \text{Cov}(X_i, X_j)
        = \text{Var}\left( \sum_{i=1}^{n} y_i X_i \right)
        \geq 0. 
    \end{align*}
    Which completes the proof.
\end{proof}
We usually require that the covariance matrix is \term{positive definite}, since if it is only positive semi-definite there are nontrivial linear combinations with 0 variance. Indeed, if 
$\SS=\begin{pmatrix}
    1 & -1 \\ 1 & -1
\end{pmatrix}$
we can check that $\var{X_1+X_2}=0$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Matrix algebra}

\term{symmetric} if $\mA^\top=\mA$

\term{orthogonal} if $\mA\mA^\top=\mA^\top\mA=\mI$

\term{eigenvalue} and \term{eigenvector} ... solution of $\det(\mA-\lambda \mI)$

also have $\det{A} = \prod_{i=1}^p \lambda_i$.

\term{Jordan decomposition} of symmetric matrtix

$$
    \mA=\mG\mL\mG^\top, \quad \mG = \TODO{}
$$

\term{Quadtraic form}: let $\mA$ symmetric, $\x$ a $(p\times 1)$ vector:
$$
    Q(\x) = \x^\top \mA \x = \sum_{i=1}^p\sum_{j=1}^p x_i A_{ij} x_j
$$
\begin{theorem}
    Transforming $\y=\mG^\top\x$ we obtain $Q(\x) = \sum_{i=1}^p \lambda_i y_i^2$
\end{theorem}
A matrix is said to be \term{positive definite} if $Q(\x)>0$ for all $\x\ne0$ and positive semi definite if $\geq$. We write $A>0$ and $A\geq 0$ respectivelly. 
\begin{theorem}
    The symmetric matrix $A$ is positive definite iff $\lambda_i > 0$ for all $i$.
\end{theorem}
From this we obtain two more usefull results. 
* If $A>0$ the inverse exists and the determinant is $>0$

* If $A>0$ there exists a unique positive definite square root with decomposition:
$$
    A^{1/2} = \mG\mL^{1/2}\mG^\top.
$$

* $\SS \geq 0$

* $\SS_{\X\Y} = \SS_{\Y\X}^\top$

* If $\X\sim(\M_{\X}, \SS_{\X\X}), \Y\sim(\M_{\Y}, \SS_{\Y\Y})$ then $\Z=(\X,\Y)^\top$ has
$$
    \SS_{\Z\Z} = \begin{pmatrix}
        \SS_{\X\X} & \SS_{\X\Y} \\ \SS{\Y\X} & \SS_{\Y\Y}
    \end{pmatrix}
$$

* Independence of $\X,\Y$ implies $\cov{\X,\Y}=\zero$. (NB: the converse not true)

* $\var{\mA\X+\boldsymbol{b}} = \mA \var{\X}\mA^\top$

* $\cov{\X+\Y, \Z} = \cov{\X, \Z} + \cov{\Y, \Z}$

* $\var{\X+\Y} = \var{\X} + \cov{\X, \Y} + \cov{\Y, \Z} + \var{\Y}$

* $\cov{\mA\X, \mB\Y} = \mA\cov{\X, \Y}\mB^\top$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection*{Lecture 3}
\subsection{Transformations}
\subsubsection{Mahalanobis transformation}

Our first transformation is the \term{Mahalanobis transformation}. We recall that we can get 0 mean and unit variance in the univariate case by the transformation $Y=\frac{X-\mu}{\s}$. In the multivariate case, suppose $X=(X_1,\dots, X_p)^\top\sim (\M, \SS)$ with $\SS$ non-singular. Then using the unique positive definite square root $\SS^{1/2}$ og $\SS$, we have the transformation:
\begin{equation}
    \boxed{\Y=\SS^{-1/2}(\X-\M) \sim (0, \mI)}
\end{equation}
\begin{proof}
    ...
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Principal component analysis (PCA)}
In the following we suppose that we observe realisations of some random vector $\X=(X_1,\dots,X_p)^\top \sim (\M, \SS)$. The goal of \term{principal component analysis} is to reduce dimensionality by removing some components and keeping components with large variance and hence more information. 


80\% of variability is explained by the components. \dots

\TODO{}


We can also do \term{empirical PCA}. Suppose that we have observations \dots

\TODO{}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{General transformations}

Suppose we transform the random vector $\X=(X_1,\dots,X_p)^\top$ into $\Y=g(\X)$. If the function $g$ is one-to-one and has differentiable inverse $u$, then 
\begin{equation*}
    f_{\Y}(\y) = |\det{\boldsymbol{J}}| f_{\X} (u(\y))
\end{equation*}
where 
$$
    \boldsymbol{J} = \brac{\frac{\partial u_i (y)}{\partial y_j}}_{i,j=1,\dots,p}
$$
is the \term{Jacobian matrix}. An important special case is that of linear transformations $\Y = \mA\X + \mb$. If $\mA$ is non-singular the inverse transform is $\X = \mA^{-1}(\Y-\mb)$ and $\boldsymbol{J} = \mA^{-1}$, so we obtain:
\begin{equation*}
    f_{\Y}(\y) = |\det{A^{-1}}| f_{\X} (\mA^{-1}(\y-\mb)).
\end{equation*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection*{Lecture 4}
\subsection{Characteristic function}

In the univariate case we often studied the \term{moment generating function}:
$$
    M_X(t) = \ev{e^{tX}}.
$$
Important properties include:
\begin{enumerate}
    \item $M_X = M_Y \Rightarrow X \overset{d}{=} Y$,
    \item $\ev{X^k} = M^{(k)}_X(0)$,
    \item Independence of $X, Y$ implies $M_{X+Y} = M_X M_Y$.
\end{enumerate}
However, it does noe exist for for instance the student-t distribution. It only exists when all moments exist. We shall now define the \term{characteristic function}, which \textbf{always} exists:
$$
    \phi_X(t) = \ev{e^{itX}}.
$$
It has similar properties:
\begin{enumerate}
    \item $\phi_X = \phi_Y \Rightarrow X \overset{d}{=} Y$,
    \item $\ev{X^k} = i^{-k} \phi^{(k)}(0)$,
    \item Independence of $X, Y$ implies $\phi_{X+Y} = \phi_X \phi_Y$.
\end{enumerate}
In the p-variate case, the functions are functions of $\mt=(t_1,\dots,t_p)^\top$. Let as usual $\X=(X_1,\dots,X_p)^\top$ be our random vector. We define:
\begin{align*}
    M_{\X}(\mt) = \ev{e^{\mt^\top \X}}, \\
    \phi_{\X}(\mt) = \ev{e^{i\mt^\top \X}}.
\end{align*}
We list some important properties.
\begin{enumerate}
    \item If $\phi_{\X}(t)$ is absolutelly integrable, the (Fourier) inversion formula holds:
    $$
        f_{\X} = \frac{1}{(2\pi)^p} \int_{\R} e^{-i\mt^\top\x} \phi_{\X}(t) dt.
    $$
    \item Denote $t_{(1)} = (t_1,\dots,0)^\top, \dots, t_{(p)}=(0,\dots,t_p)^\top$. Then 
    $$
        \phi_{\X_k}(t_k) = \ev{e^{it_1 X_1}} = \ev{e^{i\mt_{(1)}^\top \X}} =  \phi_{\X}(t_{(k)}).
    $$
    \item Let $\X=\begin{pmatrix} \X_1 \\ \X_2 \end{pmatrix}$ and $\mt=\begin{pmatrix} \mt_1 \\ \mt_2 \end{pmatrix}$ be vectors with appropriate dimensions. Then:
    \begin{equation}
        \boxed{\X_1, \X_2 \textrm{ are independent } \Leftrightarrow \phi_{\X}(\mt) = \phi_{\X_1}(\mt_1)\phi_{\X_2}(\mt_2)}
    \end{equation} 
    \item Let $\X, \Y$ be independent p-variate random vectors. Then:
    $$
        \phi_{\X+\Y}(\mt) = \phi_{\X}(\mt)\phi_{\Y}(\mt).
    $$
\end{enumerate}
We end the section with a theorem linking the distributions of 1D random variables to the distribution of the p-variate random vector $\X$:
\begin{theorem}
    \term{(Cramer-Wold)}   
    The distribution of $\X\in\R^p$ is completelly determined by the set of all 1D distributions of $\mt^\top\X, \mt\in\R^p$.
\end{theorem}
\begin{proof}
    Suppose that for all $\mt\in\R^p$ we have $\mt^\top \X \overset{d}{=} \mt^\top \Y$. Then:
    $$
        \mt^\top \X \overset{d}{=} \mt^\top \Y \Rightarrow \phi_{\mt^\top\X}(u) = \phi_{\mt^\top\Y}(u).
    $$
    Taking $u=1$ gives $\ev{e^{i\mt^\top\X}} = \ev{e^{i\mt^\top\Y}}$ for all $\mt$, so $\phi_{\X}=\phi_{\Y}$ and hence $\X \overset{d}{=}  \Y$.
\end{proof}