


%%%%%%%%%%%%%%% Lecture 9 %%%%%%%%%%%%%%%
\subsection*{Lecture 9}

Assumptions
\begin{enumerate}
    \item $\X$ is of cull column rank
    \item $E\E=\boldsymbol{0}$
    \item Homostochastic: $\var(\e_i) = 0 \quad\forall i$.
    \item If $\X$ is random, then 2 and 3 are conditioned on $\X$. 
    \item Normality of errors: $\E\sim N(0, \s^2I_n)$.
\end{enumerate}

... obtain least squares estimators $\Bh, \Sh^2$ of $\B, \S^2$

Residuals ...

\subsubsection*{Parameter estimation}

Two approaches: LSE and MLE ...

$$
    \Bh = \argmin_{\B\in\R^{k+1}} \sum_{i=1}^n (Y_i - \x_i^T\B)^2
$$

... deducing that LSE and MLE give the same result ...

...

Hat matrix

ghjfiodeifgoerjfkdworw9u0gryhj

Du fulgte ikke med nei

%%%%%%%%%%%%%%% Lecture 10 %%%%%%%%%%%%%%%
\subsection*{Lecture 10}



%%%%%%%%%%%%%%% Lecture 11 %%%%%%%%%%%%%%%
\subsection*{Lecture 11}

\begin{figure}[H] \centering
    \begin{tikzpicture}[scale=1.5]
        % Define X
        \draw[-] (0,0) -- (2,1) node[right] {$\rm{Col}(\X)$};
        % Define Y
        \draw[-] (0.6,0.3) node[below]{$\Y'$} -- (1,2) node[above] {$\Y$};
        % Projection
        \draw[-] (1,2) -- (1.6,0.8) node[below]{$\hat{\Y}$};
    \end{tikzpicture}        
\end{figure}



%%%%%%%%%%%%%%% Lecture 12 %%%%%%%%%%%%%%%
\subsection*{Lecture 12}

questions about independence. Detour into sigma algebras etc ...

\begin{theorem}
    Suppose $X, Y$ are independent random variables and that $f, g$ are two measurable functions. Then $f(X), g(Y)$ are also independent. 
\end{theorem}

ANOVA - Analysis of variance

\begin{theorem} \term{(ANOVA decomposition)}\label{thm:ANOVA}
    Assuming the necesarry assumptions, 
    $$
    \underset{\mr{SST}}{\underbrace{
        \sum_{i=1}^n (Y_i - \bar{Y})}} = 
    \underset{\mr{SSR}}{\underbrace{
        \sum_{i=1}^n (\hat{Y}_i - \bar{Y})}} + 
    \underset{\mr{SSE}}{\underbrace{
        \sum_{i=1}^n (Y_i - \hat{Y}_i)^2}}. 
    $$
\end{theorem}
\begin{proof}
    \TODO{there aint space in the margin}
\end{proof}
The 3 sums are called \term{total sum of squares}, \term{regression sum of squares} and \term{error sum of squares} respectivelly. 
This decomposition motivates the following definition. 
\begin{definition}
    The part of the total variation due to the model is called the \term{coefficient of determination} or the \term{R2-score}:
    $$
        R^2 = \frac{\mr{SSR}}{\mr{SST}} \overset{\mr{thm}} = 1 - \frac{\mr{SSE}}{\mr{SST}}.
    $$
\end{definition}
One may also prove another representation:
$$
    R^2 = \frac{\brac{\sum_{i=1}^n(Y_i-\bar{Y})(\hat{Y}_i-\bar{Y})}^2}{\sum_{i=1}^n(Y_i-\bar{Y})^2\sum_{i=1}^n(\hat{Y}_i-\bar{Y})}.
$$
This is the square of the empirical correlation between $\Y, \hat{\Y}$.

%%%%%%%%%%%%%%% Lecture 13 %%%%%%%%%%%%%%%
\subsection*{Lecture 13}


\subsubsection*{Fictional model}
"Fictional model" using $x_{ij}$ as response for some fixed feature $j$. 

\input{latex_code/tikz_figs/eq.tex}


... \TODO

\subsubsection*{General F-test}

\TODO{important to have on yellow paper}

We set up a much more general problem. Let $A\in\R^{r\times p}$, $r<p$, $\mathrm{rank}(A)=r$, $\boldsymbol{d}\in\R^d$. We test the hypothesis:
$$
    \mr{H}_0 : A\B = \bs{d}, 
    \quad\quad\quad
    \mr{H}_1 : A\B \ne \bs{d}.
$$
Some special cases of this general setup are.
\begin{enumerate}
    \item $r=1, d=0, A=(0, \dots, 1, \dots, 0)$ with $1$ at index $i$, gives the test 
    $$
        \mr{H}_0 : \b_i = 0, 
        \quad\quad\quad
        \mr{H}_1 : \b_i \ne 0.
    $$
    \item $r=1, d=0, A=(0, \dots, 1, \dots, -1, \dots, 0)$ with $1$ at index $i$ and $-1$ at index $j$, gives the test 
    $$
        \mr{H}_0 : \b_i = \b_j,
        \quad\quad\quad
        \mr{H}_1 : \b_i \ne \b_j.
    $$
    \item $r=k, d=\bs{0}\in\R^k, A=(\bs{0}, \mr{diag}(1))\in\R^{k\times p}$, gives the test 
    $$
        \mr{H}_0 : \b_i = 0 \quad\forall i\in\{1,\dots,k\},
        \quad\quad\quad
        \mr{H}_1 : \b_i \ne 0 \textrm{ for some } i\in\{1,\dots,k\}.
    $$
\end{enumerate}

%%%%%%%%%%%%%%% Lecture 14 %%%%%%%%%%%%%%%
\subsection*{Lecture 14}

Let $\mathcal{B}$ be the space of $\B$ satisfying $\mr{H}_0$. The restricted problem is:
$$
    \Bh^R = \argmin_{\B\in\mathcal{B}}(\Y-\X\B)^T(\Y-\X\B).
$$
Using lagrange multipliers and a bag of tricks, we obtain:
$$
    \Bh^R = \Bh - (\X^T\X)^{-1}\bs{A}^T(\bs{A}(\X^T\X)^{-1}\mA^T)^{-1}(\bs{A}\Bh-\bs{d}).
$$
Denoting $\Delta = \Bh-\Bh^R$, we find:
$$
    \mr{SSE}^R = \mr{SSE} + \Delta^T\X^T\X\Delta
$$

... IMPORTANT: the concrete expressions for the F statistic...

We claim that the under $\mr{H}_0$, we have

$$
    F = \frac{\mr{SSE}^R-\mr{SSE} / r}{\mr{SSE}/(n-p)} \sim F_{r, n-p}.
$$

\begin{proof}
    
    what the

\end{proof}




%%%%%%%%%%%%%%% Lecture 15 %%%%%%%%%%%%%%%
\subsection*{Lecture 15}


... example ...


\subsubsection*{Transformations of data}
Motivation:
...


box cox transformation
 

variance stabilising transformation

Suppose $\mu=\E(Y_i)$ and that $\var(Y_i)$ depends on $\mu$. ...

 

%%%%%%%%%%%%%%% Lecture 16 %%%%%%%%%%%%%%%
\subsection*{Lecture 16}

...






%%%%%%%%%%%%%%% Lecture 17 %%%%%%%%%%%%%%%
\subsection*{Lecture 17}
Suppose $k$ covariates. Then $2^k$ possible models from maximal:
$$
    Y_i = \b_0+\b_1x_{i1} + \dots + \b_kx_{ik}.
$$
to minimal:
$$
    Y_i = \b_0.
$$
We want to arrive at a compromise between simplisity and goodness of fit. 

\begin{enumerate}
    \item Adjusted coefficient of determination:
    $$
        R^2_{\mr{adj}} = 1 - \frac{\mr{SSE} / (n-k-1)}{\mr{SST} / (n-1)}
    $$
    \item 
    \item 
    \item 
\end{enumerate}
  

example...

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage\section{Multiple hypothesis testing}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


motivation ...

%%%%%%%%%%%%%%% Lecture 18 %%%%%%%%%%%%%%%
\subsection*{Lecture 18}

\dots


FWER = probability of at least one false positive finding


... two representations 


The \term{Bonferrony method}

  

The \term{Šidák method}


...

example 2019 



\dots

example 2020


%%%%%%%%%%%%%%% Lecture 19 %%%%%%%%%%%%%%%
\subsection*{Lecture 19}

Example with three groups and their means ... rewrite to regression problem ...


 

\subsubsection*{Analysis of varance (ANOVA)}
p treatments, samples ...

 


%%%%%%%%%%%%%%% Lecture 20 %%%%%%%%%%%%%%%
\subsection*{Lecture  20}


... cont ... + brief on two way ANOVA


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage\section{Design of experiment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%% Lecture 20 %%%%%%%%%%%%%%%
\subsection*{Lecture  21}
\subsection{Two level factorial design}

We suppose we have $k$ main factors $x_1,\dots,x_k$ making up a model of the form:
$$
    \Y=\beta_0 + \beta_1\x_1+\dots+\beta_k\x_k + \epsilon, \quad \epsilon\sim N(0, \sigma^2).
$$ 
Further we suppose a feature matrix $\X$ satisfying:
\begin{enumerate}
    \item Each column has entries $\pm 1$.
    \item The colomns are orthogonal, i.e. $\bf{1}^T\x_i=\sum_{i=1}^n x_{ij}=0$ and $\x_iT\x_j=n\delta_{ij}$. 
\end{enumerate}

This in particular implies that we have $\X^T \X = n I_n$. Using results from regression analysis, this significantly simplifies our estimators:

\TODO{expressions}

\begin{definition}
    The \term{main effect} of main factor $j$ is defined as:
    $$
        \textrm{effect}_j = \textrm{response at high level} - \textrm{response at low level} = 2\beta_j.
    $$
\end{definition}
The estimated effect is naturally
$$
    \widehat{\textrm{effect}_j} = \textrm{estimated response at high level} - \textrm{estimated response at low level} = 2\bh_j.
$$
To go from this to a $2^k$-design, we take into account interactions of the factors modelled as products of main factors:
$$
    Y = \b_0 + \b_1x_1+\dots+\b_kx_k + \b_{1,2}x_1x_2 + \dots+\b_{k-1,k}x_{k-1}k_{k}+\dots+\b_{1,2,\dots,k}x_1\cdots x_k.
$$
We extend the design matrix accordingly, and note that we still satisfy the assumptions. 

\TODO{example ?}

\subsubsection{Inference about effect}

Need inference about $\s^2$... cannot use estimator from multiple linear regression since for MLR we have $\sh^2=\frac{\textrm{SSE}}{n-p}$ and here $n=p$. We have to resort to one of two methods.

1. neglect some effects ... then these are normally dist ... use these as estimator \dots

2. Lenth's method ...



%%%%%%%%%%%%%%% Lecture 20 %%%%%%%%%%%%%%%
\subsection*{Lecture  22}



\subsubsection*{Resolution}

\term{resolution}


\subsubsection*{Blocking}