\section{ANOVA and Design of Experiment}
%\subsection*{Lecture 19}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Analysis of varance (ANOVA)}

%Example with three groups and their means ... rewrite to regression problem ...

From wikipedia; \hermetegn{in its simplest form, ANOVA provides a statistical test of whether two or more population means are equal, and therefore generalizes the t-test beyond two means. In other words, the ANOVA is used to test the difference between two or more means.} We will investigate this.

Suppose we have $p$ treatments/strategies, each called a \term{level}, for maximising some response (health/income etc.). Suppose further that we have $n_i$ samples of each level $i=1,\dots,p$:
\begin{align*}
    &\textrm{1\textsuperscript{st} level }     && Y_{1,1}, \dots, Y_{n_1,1} \\
    &\textrm{2\textsuperscript{nd} level }     && Y_{2,1}, \dots, Y_{n_2,2} \\
    &\quad\dots  && \\ 
    &\textrm{p\textsuperscript{th} level }     && Y_{p,1}, \dots, Y_{n_p,p}
\end{align*}
We model 
$$
    Y_{ij} = \mu_j + \epsilon_{ij}, \quad 1\leq j \leq p, \quad 1\leq i \leq n_j.
$$
We furhter assume that all $\varepsilon\sim N(0,\s^2)$ are independent. Our goal is to compare the values of $\mu$. For instance we may want to test
$$
    \mathrm{H}_0: \mu_1=\dots=\mu_p \quad\textrm{ VS }\quad \mathrm{H}_1:\textrm{not all equal}.
$$
We want to rewrite the problem so that we can use the tools we have developed for linear regression. It is then more convenient to use the following model. Let $\mu = \frac{1}{N}\sum_{j=1}^p n_j \mu_j$ be the overall average, where $N=n_1+\dots+n_p$. Denote the deviation from the overall average by $\alpha_j = \mu_j - \mu$. Our model then becomes:
$$
    Y_{ij} = \mu + \alpha_j + \epsilon_{ij}.
$$
Note further that by definition of $\mu$, we have $\sum_{j=1}^p n_j \alpha_j = 0$. In other words, one of the deviations can be expressed by the rest. In the following we simplify by assuming equal samples $n_1=\dots=n_p$. Then we can express $\alpha_p = -\alpha_1-\dots-\alpha_{p-1}$. Some more rewriting left to the reader gives the model $\Y=\X\B + \E$ where:
$$
    \B = 
    \begin{pmatrix}
        \mu \\ \alpha_1 \\ \alpha_{2} \\ \vdots \\ \alpha_{p-1}
    \end{pmatrix}
    \quad
    \X = 
    \begin{pmatrix}
        \one_n & \one_n & \zero & \hdots & \zero\\
        \one_n & \zero & \one_n & \hdots & \zero\\
        \vdots & \vdots & \vdots & \ddots & \vdots\\
        \one_n & -\one_n & -\one_n & \hdots & -\one_n\\
    \end{pmatrix}
    .
$$
Our hypothesis test is equivalent to 
$$
    \mathrm{H}_0: \alpha_1=\dots=\alpha_{p-1}=0 \quad\textrm{ VS }\quad \mathrm{H}_1:\textrm{not all }0.
$$
Hence, we may use the $F$-test that we developed in \Cref{sec:F_test}. Using the general $F$-test we may test many other as well.
 %\subsection*{Lecture  20}
\subsubsection{Two-way ANOVA}
We may also have $2$ levels both present at the same time. Our model is then:
$$
    Y_{ij} = \mu + \alpha_i + \beta_j + \epsilon_{ij}, \quad i=1,\dots,r, \quad j=1,\dots ,s.
$$
Adding replicates at the different combinations of $i,j$ we have
$$
    Y_{ij} = \mu + \alpha_i + \beta_j + \epsilon_{ijk}, \quad k=1,\dots,n_{ij}.
$$
Finally we may add an interaction term between the two:
$$
    Y_{ij} = \mu + \alpha_i + \beta_j + \gamma_{ij} + \epsilon_{ijk}.
$$
Also here we have constraints:
$$
    \sum_{i=1}^r \alpha_i = \sum_{j=1}^s \beta_j = \sum_{i=1}^r \gamma_{ij} = \sum_{j=1}^s \gamma_{ij} = 0.
$$

%\subsection*{Lecture  21}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Two level factorial design}

%In this section we present ideas from the note \cite{Tyssedal}. 
We suppose we have $k$ main factors $x_1,\dots,x_k$ making up a model of the form:
$$
    \Y=\beta_0 + \beta_1\x_1+\dots+\beta_k\x_k + \epsilon, \quad \epsilon\sim N(0, \sigma^2).
$$ 
We only allow for our covariates $x_i$ to be at \term{low level} and \term{high level}. This is moddeled as $-1, +1$, and is the first of the following assumptions. The second assumption will ensure simple computation and the most precise inference about coefficients:
\begin{enumerate}
    \item Each column has entries $\pm 1$.
    \item The colomns are orthogonal, i.e. $\one^T\x_i=\sum_{i=1}^n \x_{ij}=0$ and $\x_i^T \x_j=n\delta_{ij}$. 
\end{enumerate}
This in particular implies that we have $\X^T \X = n I_n$. Using results from regression analysis, this significantly simplifies our estimators. We get $\Bh=\frac{1}{n} \X^T\Y$:
\begin{equation*}
    \b_j = \frac{1}{n} \sum_{i=1}^n x_{ij} Y_j.
\end{equation*}
The estimate of the $j$'th coefficient only depents on the $j$'th covariate! Hence removal of covariates leaves the rest unchanged. We call the covariates $x_1,\dots, x_k$ the main factors. A central question in experimentation is what happens when a factor goes from high to low level. This motivates the definition:
\begin{definition}
    The \term{main effect} of main factor $j$ is defined as:
    \begin{align*}
        \textrm{effect}_j 
        &= \textrm{expected response at high level} - \textrm{expected response at low level} 
        \\&= \ev{Y | x_{j} = 1} - \ev{Y | x_{j} = -1}
        = 2\beta_j.        
    \end{align*}
\end{definition}
The estimated effect is naturally by:
\begin{align*}
    2\bh_j 
    &= \frac{1}{n/2} \sum_{x_{ij} = 1} Y_i - \frac{1}{n/2} \sum_{x_{ij} = - 1} Y_i  
    \\&= \textrm{estimated response at high level} - \textrm{estimated response at low level} 
    =: \widehat{\textrm{effect}_j}
\end{align*}
Another consequence of the second assumption is that $\Bh \sim N\brac{\B, \frac{\s^2}{n}I}$. Hence the components are independent with the same variance $\s^2/n$. 

We also remark that we may express the regression sum of squares as:
\begin{align*}
    \textrm{SSR} 
    &= \sum_{i=1}^{n} (\hat{y_i} - \bar{y})^2 
    = \sum_{i=1}^{n} \left( \sum_{j=0}^{n} x_{ij} \hat{\beta}_j - \hat{\beta}_0 \right)^2 
    \\&= \sum_{i=1}^{n} \left( \sum_{j=1}^{n} x_{ij} \hat{\beta}_j + \overbrace{x_{i0}}^{=1} \hat{\beta}_0 - \bh_0 \right)^2
    = (\X^* \Bh^*)^T (\X^* \Bh^*)
    \\&= \Bh^{*T} \X^{*T} \X^* \Bh^* 
    = \Bh^{*T} n I_k \Bh^* 
    = n \sum_{j=1}^{n} \bh_j^2.
\end{align*}
Here $\X^*$ is the feature matrix without the first column $\one$. The expression shows that $n\bh_j$ may be interpreted as the amount of variability explained by factor $j$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Modelling interactions}

To go from this to a \term{$2^k$-design}, we take into account all interactions of the factors. These are modelled as products of main factors:
$$
    Y = \b_0 + \b_1x_1+\dots+\b_kx_k + \b_{1,2}x_1x_2 + \dots+\b_{k-1,k}x_{k-1}k_{k}+\dots+\b_{1,2,\dots,k}x_1\cdots x_k.
$$
We extend the design matrix accordingly, and note that we still satisfy the assumptions. Hence we still have $\Bh = \frac{1}{n}\X^\top\Y$. 

We define the \term{interaction between two factors} as half the main effect of one when the other is at high level minus half the main effect when the other is at low. This encapsulates that if the change in respons between levels of factor A is not independent of the level of B there is an interaction. Given the definition, it is natural to get the \term{estimated interaction effect} of 1 and 2 as $2\bh_{1,2}$. To see that this is natural, rewrite to obtain:
\begin{align*}
    2\bh_{1,2} 
    &= \frac{1}{1/2}\underbrace{\brac{\frac{1}{n/2}\sum_{\substack{x_{i1} = 1 \\ x_{i2} =  1}} Y_i - \frac{1}{n/2}\sum_{\substack{x_{i1} = -1 \\ x_{i2} =  1}} Y_i}}_{\textrm{Estimated effect of 1 with 2 held at high}}
    -\frac{1}{1/2}\underbrace{\brac{\frac{1}{n/2}\sum_{\substack{x_{i1} = 1 \\ x_{i2} = -1}} Y_i - \frac{1}{n/2}\sum_{\substack{x_{i1} = -1 \\ x_{i2} = -1}} Y_i}}_{\textrm{Estimated effect of 1 with 2 held at low}}
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Inference about effect}

We have seen that $\hat{\textrm{Effect}_j} \sim N(\textrm{Effect}_j, \frac{4\s^2}{n})$. To make inference about the effect/coefficients we need to make inference about $\s^2$. We cannot use estimator from multiple linear regression since for MLR we have $\sh^2=\frac{\textrm{SSE}}{n-p}$ and here $n=p$. If we don't have replicates in our experiment, to increase $n$, we have to resort to one of two methods.
\begin{enumerate}
    \item We can neclect $m$ higher order factors where it is reasonable that $\textrm{Effect}_j = 0$. Then we have $\hat{\textrm{Effect}_j} \sim N(0, \frac{4\s^2}{n})$. Letting $\s^2_{\textrm{effect}} = \frac{4\s^2}{n}$ we then estimate:
    $$
        \sh^2_{\textrm{effect}} = \frac{1}{m} \sum_{\textrm{negligable}} \hat{\textrm{Effect}}_j^2
    $$

    \item The idea of \term{Lenth's method} / \term{pseudo standard error} (PSE) is to compute:
    $$
        C_1, \ldots, C_m \text{ - the estimated effects } \hat{A}, \hat{B}, \hat{AB}, \ldots
    $$  
    We then do the following:
        \begin{enumerate}
            \item Order $|C_j|$ in increasing order.
            \item Find the median $M$ of $|C_1|, \ldots, |C_{m}|$ and compute $\delta_0 = 1.5M$.
            \item Remove the effects $C_j$ with $|C_j| \geq 2.5 \delta_0$ and find the median of the rest of $|C_j|$.
        \end{enumerate}
    Having done this we compute the PSE as 
    $$
        PSE = 1.5 \cdot \text{median} \left\{ |C_j| : |C_j| < 2.5 \delta_0 \right\}.
    $$
    This is then used as our estimator; $\hat{\sigma}_{PSE} = PSE$.
        
\end{enumerate}

In the case of replications ($n\cdot 2^k$ experiment) we use results from multiple linear regression:
$$
    \sh^2 = \frac{SSE}{n-2^k}, \quad \frac{\bh_j}{\sh / \sqrt{n}} \sim t_{n-2^k}.
$$


%\subsection*{Lecture  22}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Fractional factorial design ($2^{k-r}$-design)}

One common fractional design is the \term{half fraction}, where we fullfill only half of the $2^k$ experiments of the full design. To choose experiments perform:
\begin{enumerate}
    \item Choose $k-1$ main factors and consider their full $2^{k-1}$ design.
    \item For the $k$-th main factor, assign the same values as one of the interactions in the above design. 
\end{enumerate}
Suppose for instance we are performing a $2^{4-1}$ design with main factors A, B, C and D. Then to the full design for A, B, C and let:
$$
    D = ABC.
$$
We call the above the \term{generator of the design}. Multiplication of this equation by D gives:
$$
    1 = ABCD.
$$
This is called the \term{defining relation}. From the generator or from the defining relation we may obtain the remaining \term{aliases} / \term{confounded factors} by multiplying both sides of $1=ABCD$ by A, B, C, AB, AC, BC to obtain:
\begin{align*}
        A &= BCD \quad &&B = ACD \quad &&&C = ABD \\
        AB &= CD \quad &&AC = BD \quad &&&BC = AD.
\end{align*}
\term{generator}
The minimal order of $LHS+RHS$ is called the \term{resolution} of the design. Here the resolution is IV. The greater the resolution the better. This is because at high resolution we ensure that the main factors are only confounded by higher order interactions. We note that when estimating effects, the estimator of confounded factors is the same, in our example for instance:
$$
    \hat{D} = \hat{ABC}.
$$
This will encapsulate the joint effect of both D and ABC, which again shows that high resolution is good as we often neglect the higher order interactions anyways. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Blocking}

The motivation for \term{blocking} is that the conditions of the experiments may change. Hence we want a clever way of deciding which experiments to to \hermetegn{today} etc. Suppose that there is a fixed change $h$ to the response, i.e. that \hermetegn{tomorrow} we have
$$
    Y_i \mapsto Y_i + h.
$$
If we choose experiments such that for a given factor, the number of high and low level ($+$ and $-$) are equal, the effect of this factor is unchanged by the added $h$. We usually consider the main factors of greatest importance. Consider for example a $2^3$ experiment done over 2 days. We say it is divided in 2 \term{blocks}. Let block I be done with ABC at $-$ and block II at $+$. We obtain the following:

\begin{figure}[H]\centering
    \begin{tabular}{*7c}
        \multicolumn{7}{c}{\textbf{Block I}}\\
        \toprule
        A & B & C & AB & AC & BC & ABC\\
        \midrule
        + & + & - & + & - & - & - \\
        + & - & + & - & + & - & - \\
        - & + & + & - & - & + & - \\
        - & - & - & + & + & + & - \\
        \bottomrule
    \end{tabular}        
        \quad
    \begin{tabular}{*7c}
        \multicolumn{7}{c}{\textbf{Block II}}\\        \toprule
        A & B & C & AB & AC & BC & ABC \\
        \midrule
        + & + & + & + & + & + & + \\
        + & - & - & - & - & + & + \\
        - & - & + & + & - & - & + \\
        - & + & - & - & + & - & + \\
        \bottomrule
    \end{tabular}        
    \caption{The two blocks when partitioning by the sign of ABC.}
\end{figure}

In this example our choice is good. Only the 3-factor interaction is affected by the supposed change in response since all other have equally many $+$ and $-$. It would be much wose if a main factor were confounded with the block effect. 